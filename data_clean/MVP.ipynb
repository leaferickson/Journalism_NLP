{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "# from nlp_pipeline import nlp_preprocessor\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "class nlp_preprocessor:\n",
    "   \n",
    "    def __init__(self, vectorizer=CountVectorizer(), tokenizer=None, \n",
    "                 cleaning_function=None, stemmer=None):\n",
    "        \"\"\"\n",
    "        A class for pipelining our data in NLP problems. The user provides a series of \n",
    "        tools, and this class manages all of the training, transforming, and modification\n",
    "        of the text data.\n",
    "        ---\n",
    "        Inputs:\n",
    "        vectorizer: the model to use for vectorization of text data\n",
    "        tokenizer: The tokenizer to use, if none defaults to split on spaces\n",
    "        cleaning_function: how to clean the data, if None, defaults to the in built class\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = self.splitter\n",
    "        if not cleaning_function:\n",
    "            cleaning_function = self.clean_text\n",
    "        self.stemmer = stemmer\n",
    "        self.tokenizer = tokenizer\n",
    "#        self.model = model\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_fit = False\n",
    "        self. words = None\n",
    "        \n",
    "    def splitter(self, text):\n",
    "        \"\"\"\n",
    "        Default tokenizer that splits on spaces naively\n",
    "        \"\"\"\n",
    "        return text.split(' ')\n",
    "        \n",
    "    def clean_text(self, text, tokenizer, stemmer):\n",
    "        \"\"\"\n",
    "        A naive function to lowercase all works can clean them quickly.\n",
    "        This is the default behavior if no other cleaning function is specified\n",
    "        \"\"\"\n",
    "        cleaned_text = []\n",
    "        for post in text:\n",
    "            cleaned_words = []\n",
    "            for word in tokenizer(post[\"tweet\"]):\n",
    "                low_word = word.lower()\n",
    "                if stemmer:\n",
    "                    low_word = stemmer.stem(low_word)\n",
    "                cleaned_words.append(low_word)\n",
    "            cleaned_text.append(' '.join(cleaned_words))\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the data and then fits the vectorizer with\n",
    "        the user provided text\n",
    "        \"\"\"\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        self.vectorizer.fit(clean_text)\n",
    "        self._is_fit = True\n",
    "        self. words = self.vectorizer.get_feature_names()\n",
    "        \n",
    "    def get_words(self):\n",
    "        if self._is_fit == False:\n",
    "            return \"Not yet fit\"\n",
    "        return self.words\n",
    "        \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        Cleans any provided data and then transforms the data into\n",
    "        a vectorized format based on the fit function. Returns the\n",
    "        vectorized form of the data.\n",
    "        \"\"\"\n",
    "        if not self._is_fit:\n",
    "            raise ValueError(\"Must fit the models before transforming!\")\n",
    "        clean_text = self.cleaning_function(text, self.tokenizer, self.stemmer)\n",
    "        return self.vectorizer.transform(clean_text)\n",
    "    \n",
    "    def save_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        pickle.dump(self.__dict__, open(filename+\".mdl\",'wb'))\n",
    "        \n",
    "    def load_pipe(self, filename):\n",
    "        \"\"\"\n",
    "        Writes the attributes of the pipeline to a file\n",
    "        allowing a pipeline to be loaded later with the\n",
    "        pre-trained pieces in place.\n",
    "        \"\"\"\n",
    "        if type(filename) != str:\n",
    "            raise TypeError(\"filename must be a string\")\n",
    "        if filename[-4:] != '.mdl':\n",
    "            filename += '.mdl'\n",
    "        self.__dict__ = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Mongo DB\n",
    "IP = \"\" #Put this in quotes as a string\n",
    "\n",
    "client = MongoClient(\"mongodb://user_name:pass_word@%s/project4\" % IP) # defaults to port 27017\n",
    "\n",
    "db = client.project4\n",
    "\n",
    "cursor = db.project_collection.find()\n",
    "\n",
    "data = []\n",
    "for i in range(1000):\n",
    "    data.append(cursor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp_preprocessor(tokenizer=TweetTokenizer().tokenize, cleaning_function=None, stemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fit(data[0:100])\n",
    "vectorized_docs = test.transform(data[0:100]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vectorized_docs).sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"journalism_tweets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stemmer, stopwords = set(stopwords.words('english'))):\n",
    "    \"\"\"\n",
    "    A naive function to lowercase all works can clean them quickly.\n",
    "    This is the default behavior if no other cleaning function is specified\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        for word in tokenizer(post[\"tweet\"][2:]):\n",
    "            word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE)\n",
    "            if word == \"\" or len(word) == 1:\n",
    "                continue\n",
    "            low_word = word.lower()\n",
    "            if word_tokenize(low_word)[0].isalpha():\n",
    "                if low_word not in stopwords:\n",
    "                    cleaned_words.append(low_word)\n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer().tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "cleaned_text = []\n",
    "for post in data[0:100]:\n",
    "    cleaned_words = []\n",
    "    for word in tokenizer(post[\"tweet\"][2:]):\n",
    "        word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE)\n",
    "        if word == \"\" or len(word) == 1:\n",
    "            continue\n",
    "        low_word = word.lower()\n",
    "        if word_tokenize(low_word)[0].isalpha():\n",
    "            if low_word not in stop_words:\n",
    "                print(low_word)\n",
    "                cleaned_words.append(low_word)\n",
    "    cleaned_text.append(' '.join(cleaned_words))\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp_preprocessor(tokenizer=TweetTokenizer().tokenize, cleaning_function=clean_text, stemmer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fit(data[0:100])\n",
    "vectorized_docs2 = test.transform(data[0:100]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    1\n",
       "3    2\n",
       "4    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectorized_docs2).sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'abortionist',\n",
       " 'abortions',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'acceptance',\n",
       " 'accuser',\n",
       " 'acid',\n",
       " 'actually',\n",
       " 'administrator',\n",
       " 'admits',\n",
       " 'advice',\n",
       " 'alex',\n",
       " 'allegations',\n",
       " 'allegiance',\n",
       " 'alone',\n",
       " 'already',\n",
       " 'also',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'americans',\n",
       " 'andrew',\n",
       " 'angeles',\n",
       " 'angry',\n",
       " 'annog',\n",
       " 'anthem',\n",
       " 'anyone',\n",
       " 'areas',\n",
       " 'aretha',\n",
       " 'article',\n",
       " 'asked',\n",
       " 'asks',\n",
       " 'assault',\n",
       " 'assisted',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'audience',\n",
       " 'australia',\n",
       " 'back',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'ban',\n",
       " 'banned',\n",
       " 'barbara',\n",
       " 'barring',\n",
       " 'ben',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'bin',\n",
       " 'biologically',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bishops',\n",
       " 'blasting',\n",
       " 'body',\n",
       " 'boycott',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'brennan',\n",
       " 'cakeshop',\n",
       " 'calls',\n",
       " 'campus',\n",
       " 'candidate',\n",
       " 'captor',\n",
       " 'cars',\n",
       " 'cash',\n",
       " 'catholic',\n",
       " 'causing',\n",
       " 'celebrates',\n",
       " 'chambers',\n",
       " 'chicago',\n",
       " 'child',\n",
       " 'chose',\n",
       " 'church',\n",
       " 'cia',\n",
       " 'classified',\n",
       " 'clearance',\n",
       " 'clinton',\n",
       " 'close',\n",
       " 'closest',\n",
       " 'club',\n",
       " 'collusion',\n",
       " 'comes',\n",
       " 'comey',\n",
       " 'coming',\n",
       " 'commander',\n",
       " 'comment',\n",
       " 'commission',\n",
       " 'condemns',\n",
       " 'consent',\n",
       " 'constitutional',\n",
       " 'containing',\n",
       " 'contestant',\n",
       " 'control',\n",
       " 'controlling',\n",
       " 'conway',\n",
       " 'correction',\n",
       " 'cowards',\n",
       " 'criticizes',\n",
       " 'crowd',\n",
       " 'culture',\n",
       " 'cuomo',\n",
       " 'curl',\n",
       " 'cutting',\n",
       " 'cynthia',\n",
       " 'daily',\n",
       " 'dallas',\n",
       " 'dark',\n",
       " 'day',\n",
       " 'defeats',\n",
       " 'defecation',\n",
       " 'deleted',\n",
       " 'democrat',\n",
       " 'democrats',\n",
       " 'destroys',\n",
       " 'detect',\n",
       " 'died',\n",
       " 'director',\n",
       " 'dirty',\n",
       " 'discrimination',\n",
       " 'discussion',\n",
       " 'dishonest',\n",
       " 'disney',\n",
       " 'dnc',\n",
       " 'documents',\n",
       " 'dog',\n",
       " 'dolphins',\n",
       " 'doom',\n",
       " 'dorsey',\n",
       " 'double',\n",
       " 'drags',\n",
       " 'drug',\n",
       " 'dunham',\n",
       " 'echo',\n",
       " 'economy',\n",
       " 'edgy',\n",
       " 'editorials',\n",
       " 'ellison',\n",
       " 'endorsement',\n",
       " 'enemy',\n",
       " 'ep',\n",
       " 'episode',\n",
       " 'equal',\n",
       " 'er',\n",
       " 'eradicate',\n",
       " 'escapes',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'excited',\n",
       " 'explains',\n",
       " 'explosives',\n",
       " 'eye',\n",
       " 'faces',\n",
       " 'facts',\n",
       " 'fake',\n",
       " 'fan',\n",
       " 'faux',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'feels',\n",
       " 'feminist',\n",
       " 'fighting',\n",
       " 'film',\n",
       " 'finally',\n",
       " 'finishes',\n",
       " 'fire',\n",
       " 'fired',\n",
       " 'fires',\n",
       " 'firing',\n",
       " 'first',\n",
       " 'flagship',\n",
       " 'florida',\n",
       " 'flushed',\n",
       " 'fo',\n",
       " 'former',\n",
       " 'forward',\n",
       " 'franklin',\n",
       " 'friends',\n",
       " 'georgia',\n",
       " 'germany',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'girl',\n",
       " 'give',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glenn',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'gop',\n",
       " 'gosnell',\n",
       " 'got',\n",
       " 'grabs',\n",
       " 'great',\n",
       " 'ground',\n",
       " 'guess',\n",
       " 'gunn',\n",
       " 'handle',\n",
       " 'happy',\n",
       " 'harassing',\n",
       " 'hardest',\n",
       " 'harrowing',\n",
       " 'head',\n",
       " 'headline',\n",
       " 'health',\n",
       " 'here',\n",
       " 'hero',\n",
       " 'hilarious',\n",
       " 'hillary',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hold',\n",
       " 'honoring',\n",
       " 'house',\n",
       " 'howdy',\n",
       " 'husband',\n",
       " 'ice',\n",
       " 'ideas',\n",
       " 'identity',\n",
       " 'imposes',\n",
       " 'imposing',\n",
       " 'insane',\n",
       " 'install',\n",
       " 'installment',\n",
       " 'integrity',\n",
       " 'intense',\n",
       " 'investigation',\n",
       " 'iphone',\n",
       " 'isis',\n",
       " 'issues',\n",
       " 'jack',\n",
       " 'jam',\n",
       " 'james',\n",
       " 'joined',\n",
       " 'jones',\n",
       " 'justice',\n",
       " 'keith',\n",
       " 'kellyanne',\n",
       " 'key',\n",
       " 'khan',\n",
       " 'killed',\n",
       " 'klavan',\n",
       " 'kneels',\n",
       " 'knife',\n",
       " 'knowles',\n",
       " 'kurdish',\n",
       " 'labeled',\n",
       " 'laden',\n",
       " 'language',\n",
       " 'lara',\n",
       " 'lashes',\n",
       " 'last',\n",
       " 'laughs',\n",
       " 'law',\n",
       " 'lay',\n",
       " 'leaker',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'lefties',\n",
       " 'legendary',\n",
       " 'lena',\n",
       " 'lgbt',\n",
       " 'life',\n",
       " 'light',\n",
       " 'listen',\n",
       " 'live',\n",
       " 'loaded',\n",
       " 'london',\n",
       " 'longest',\n",
       " 'los',\n",
       " 'made',\n",
       " 'makes',\n",
       " 'male',\n",
       " 'marital',\n",
       " 'masterpiece',\n",
       " 'matter',\n",
       " 'maxine',\n",
       " 'media',\n",
       " 'meet',\n",
       " 'mentioned',\n",
       " 'miami',\n",
       " 'mike',\n",
       " 'minutes',\n",
       " 'misrepresented',\n",
       " 'miss',\n",
       " 'missouri',\n",
       " 'mob',\n",
       " 'molesting',\n",
       " 'money',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'must',\n",
       " 'national',\n",
       " 'navy',\n",
       " 'nderif',\n",
       " 'netflix',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nf',\n",
       " 'nfull',\n",
       " 'nhttps',\n",
       " 'night',\n",
       " 'nixon',\n",
       " 'noah',\n",
       " 'notes',\n",
       " 'nsa',\n",
       " 'nudes',\n",
       " 'number',\n",
       " 'nwatch',\n",
       " 'obama',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'omarosa',\n",
       " 'opening',\n",
       " 'opponent',\n",
       " 'org',\n",
       " 'original',\n",
       " 'oscars',\n",
       " 'outlets',\n",
       " 'outrage',\n",
       " 'overdoses',\n",
       " 'oversaw',\n",
       " 'parental',\n",
       " 'parking',\n",
       " 'pas',\n",
       " 'pearl',\n",
       " 'pedophilia',\n",
       " 'pelosi',\n",
       " 'pence',\n",
       " 'penned',\n",
       " 'pennsylvania',\n",
       " 'people',\n",
       " 'permit',\n",
       " 'peter',\n",
       " 'philips',\n",
       " 'phillips',\n",
       " 'pirate',\n",
       " 'plaguing',\n",
       " 'plans',\n",
       " 'plastic',\n",
       " 'play',\n",
       " 'pledge',\n",
       " 'podcast',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'politics',\n",
       " 'polling',\n",
       " 'poop',\n",
       " 'pooperindent',\n",
       " 'porn',\n",
       " 'poster',\n",
       " 'praises',\n",
       " 'prayer',\n",
       " 'precincts',\n",
       " 'predators',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'priests',\n",
       " 'principles',\n",
       " 'proclaimed',\n",
       " 'progressive',\n",
       " 'promotes',\n",
       " 'protest',\n",
       " 'public',\n",
       " 'publish',\n",
       " 'publishes',\n",
       " 'queen',\n",
       " 'queer',\n",
       " 'questionabl',\n",
       " 'questions',\n",
       " 'racial',\n",
       " 'racist',\n",
       " 'radio',\n",
       " 'raid',\n",
       " 'rainbows',\n",
       " 'rate',\n",
       " 'reaffirms',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'red',\n",
       " 'reduce',\n",
       " 'regarding',\n",
       " 'releases',\n",
       " 'reminder',\n",
       " 'remove',\n",
       " 'report',\n",
       " 'reprehensible',\n",
       " 'republicans',\n",
       " 'required',\n",
       " 'reveals',\n",
       " 'revoke',\n",
       " 'revoking',\n",
       " 'rhodes',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'rise',\n",
       " 'robot',\n",
       " 'role',\n",
       " 'rt',\n",
       " 'run',\n",
       " 'sanctions',\n",
       " 'sanders',\n",
       " 'santa',\n",
       " 'sarah',\n",
       " 'satisfied',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scandal',\n",
       " 'scanners',\n",
       " 'scene',\n",
       " 'school',\n",
       " 'screen',\n",
       " 'seal',\n",
       " 'seats',\n",
       " 'security',\n",
       " 'see',\n",
       " 'selfie',\n",
       " 'senate',\n",
       " 'sending',\n",
       " 'sentence',\n",
       " 'sex',\n",
       " 'shame',\n",
       " 'shelter',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'since',\n",
       " 'slammed',\n",
       " 'slams',\n",
       " 'slurs',\n",
       " 'snarky',\n",
       " 'social',\n",
       " 'socialists',\n",
       " 'sorrow',\n",
       " 'soul',\n",
       " 'spare',\n",
       " 'speech',\n",
       " 'staggering',\n",
       " 'standard',\n",
       " 'star',\n",
       " 'statement',\n",
       " 'statements',\n",
       " 'station',\n",
       " 'steps',\n",
       " 'steven',\n",
       " 'stir',\n",
       " 'straws',\n",
       " 'street',\n",
       " 'strzok',\n",
       " 'student',\n",
       " 'subway',\n",
       " 'suicide',\n",
       " 'surrendered',\n",
       " 'suspension',\n",
       " 'system',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'targeted',\n",
       " 'teacher',\n",
       " 'teg',\n",
       " 'texas',\n",
       " 'time',\n",
       " 'title',\n",
       " 'today',\n",
       " 'tonight',\n",
       " 'tough',\n",
       " 'town',\n",
       " 'trailer',\n",
       " 'trans',\n",
       " 'transgenders',\n",
       " 'trevor',\n",
       " 'tribune',\n",
       " 'tries',\n",
       " 'troll',\n",
       " 'trolling',\n",
       " 'trolls',\n",
       " 'trump',\n",
       " 'trying',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'tweet',\n",
       " 'tweeting',\n",
       " 'twitter',\n",
       " 'tyler',\n",
       " 'unicorns',\n",
       " 'union',\n",
       " 'uses',\n",
       " 'vatican',\n",
       " 'vi',\n",
       " 'video',\n",
       " 'violation',\n",
       " 'vows',\n",
       " 'walsh',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'warriors',\n",
       " 'watch',\n",
       " 'waters',\n",
       " 'weekly',\n",
       " 'white',\n",
       " 'winning',\n",
       " 'without',\n",
       " 'witness',\n",
       " 'women',\n",
       " 'xef',\n",
       " 'year']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
